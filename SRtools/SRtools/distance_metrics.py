"""
Distance metrics and comparison functions for SR models.

This module contains distance calculation functions moved from SRmodellib.py
for better code organization and maintainability.
"""

import numpy as np
from scipy.stats import ks_2samp, entropy


def ks_test(distributions1, distributions2, bins, metric ='test_stat', trim=0,full=False):
    #returns the ks test statistic and p-value between two distributions
    #metric can be 'test_stat', 'p_value', mse or KL_divergence
    #trim is the number of bins to ignore at the begining of the distribution
    n = len(distributions1)
    distances = np.zeros(n)
    dx = bins[1]-bins[0]
    for i, key in enumerate(distributions1.keys()):
        dist1 = distributions1[key][trim:]
        dist2 = distributions2[key][trim:]
        dist1 = dist1/np.sum(dist1*dx)
        dist2 = dist2/np.sum(dist2*dx)
        ks = ks_2samp(dist1, dist2)
        if metric == 'test_stat':
            distances[i] = ks.statistic
        elif metric == 'p_value':
            distances[i] = 1-ks.pvalue
        elif metric == 'mse':
            # print(key,':',np.mean((dist1-dist2)**2))
            distances[i] = np.mean((dist1-dist2)**2)
        elif metric == 'KL_divergence':
            distances[i] = entropy(dist1,dist2)
        else:
            raise ValueError('metric should be test_stat or p_value')
    if full:
        return distances
    return np.sqrt(np.sum(distances)/n)
    
def trim_to_range(t,vals,time_range,renormalize_survival = False):
    #trims the values to the time range
    i_start = np.abs(t - time_range[0]).argmin()
    i_end = np.abs(t - time_range[1]).argmin()
    if renormalize_survival:
        vals = vals/vals[i_start]
    return t[i_start:i_end], vals[i_start:i_end]


def baysianDistance(sr1, sr2, time_range=None, dt =1, debug = False):
    """
    Calculate the likelihood that the death times of sr1 are generated by sr2.
    convention is that sr1 is the data and sr2 is the simulation
    """
    from statsmodels.nonparametric.kernel_density import KDEMultivariate
    
    #if number of deathtimes is too small that causes issues and anyways not probable as a legitimate parameter set
    if len(sr2.getDeathTimes()) <= 5:
        return -np.inf
    
    death_times2 = sr2.getDeathTimes()
    events2 = sr2.events
    #if time range is not None, use only deathtimes withtin the time range
    if time_range is not None:
        events2 = events2[(death_times2 >= time_range[0]) & (death_times2 <= time_range[1])]
        death_times2 = death_times2[(death_times2 >= time_range[0]) & (death_times2 <= time_range[1])]
    #check that there are enough events to calculate the likelihood meaningfully
    if np.sum(events2) <= 5:
        return -np.inf
    
    death_times2 = death_times2[events2==1] #only those who died
    #this would be a smooth distribution generatated from sr2(simulation) to sample sr1(data) from
    kde = KDEMultivariate(death_times2, var_type='c', bw='normal_reference')
    

    events = sr1.events
    death_times = sr1.getDeathTimes()
    dts =sr1.properties.get('dt',np.ones_like(death_times)*dt)
    if time_range is not None:
        events = events[(death_times >= time_range[0]) & (death_times <= time_range[1])]
        dts = dts[(death_times >= time_range[0]) & (death_times <= time_range[1])]
        death_times = death_times[(death_times >= time_range[0]) & (death_times <= time_range[1])]
    died = death_times[events==1]
    censored = death_times[events==0]
    ndied = len(died)
    p_death_before_t_end =np.sum(events2)/len(events2)
    
    
    #creating the list of times we need to evalutate the kde at. This is all the death times + all the death times - the relevant dt
    # Creating the list of times we need to evaluate the kde at.
    # This includes all the death times and all the death times minus the relevant dt.
    evaluation_times = np.concatenate((death_times, death_times - dts))
    evaluation_times = evaluation_times[(evaluation_times >= 0) & (evaluation_times <= max(death_times))]
    evaluation_times = np.unique(evaluation_times)

    #this piece of code is to accelerate the loglikelihood calculation
    times = evaluation_times
    log_pdt = kde.cdf(times) #the log integral of the probability density function on the time grid
    log_pdt = np.log(log_pdt[1:]-log_pdt[:-1])
    #insert a 0 at the beginning of the log_pdt array to make it the same length as times
    log_pdt = np.insert(log_pdt, 0, 0)
    

    #if logp has nan values then try again then raise an error to debug
    if np.any(np.isnan(log_pdt)):
        if debug:
            print('log_pdt has nan values')
        return np.NaN

    logcdf = np.log(1-kde.cdf(times)) 
    #for every time in death times, find the nearst index in times and get the logp
    logps = 0
    logps_censored = 0
    for t in died:
        idx = np.argmin(np.abs(times-t))
        logps+=(log_pdt[idx])
        if debug:
            if log_pdt[idx] == -np.inf or np.isnan(log_pdt[idx]):
                print(f'log_pdt[{idx}] is {log_pdt[idx]}')

    for t in censored:
        idx = np.argmin(np.abs(times-t))
        logps_censored+=(logcdf[idx])

    #the liklihod given by the kde is L= p(t_death=x_i)|died before t_end) so to correct we need to multiply by the number of ndied/n
    #in the loglikelihood this gives a term of ndied*np.log(p_death_before_t_end)
    sums = logps+ndied*np.log(p_death_before_t_end) +logps_censored
    #check if sums is nan if so then raise an error to debug.
    if np.isnan(sums):
        print('ndied:',ndied)
        print('p_death_before_t_end:',p_death_before_t_end)
        print('logps:',logps)
        print('censored:',censored)
        print('censored_logLiklihood(censored):',logps_censored)
        raise ValueError('sums is nan')
    if debug:
        print('ndied:',ndied)
        print('p_death_before_t_end:',p_death_before_t_end)
        print('logps:',logps)
        print('censored:',censored)
        print('censored_logLiklihood(censored):',logps_censored)
        print('sums:',sums)
    return sums


def baysian_dirichlet_distance(sr1, sr2, time_range=None, dt=1, debug=False, lambda_smooth=0.5):
    """
    Calculate the Dirichlet-multinomial likelihood that the death times of sr1 are generated by sr2.
    
    This function uses a Dirichlet-multinomial model to compute the log-likelihood.
    Convention: sr1 is the data, sr2 is the simulation.
    
    The likelihood is computed using the Dirichlet-multinomial formula:
        log L = gammaln(A0) - gammaln(m + A0) + sum(gammaln(y + alpha) - gammaln(alpha))
    
    where:
        - alpha = c + lambda_smooth (counts from simulation + smoothing parameter)
        - y = counts from data
        - A0 = sum(alpha) (sum of all posterior alphas)
        - m = sum(y) (total data observations)
    
    For censored data, we add:
        log L_cens = sum over bins r [gammaln(A[r] + L[r]) - gammaln(A[r])
                                      - gammaln(A0 + T_after + L[r]) + gammaln(A0 + T_after)]
    where:
        - A[r] = sum_{i>=r} alpha_prime[i] (tail cumulative sum)
        - L[r] = number of right-censored observations with censoring time in bin r
        - T_after = cumulative count of censored after bin r
        - alpha_prime = alpha + y (posterior parameters)
    
    Parameters:
    -----------
    sr1 : SR object
        The data SR object (observations)
    sr2 : SR object
        The simulation SR object (model to evaluate)
    time_range : tuple, optional
        Time range (min, max) to restrict analysis
    dt : float or array-like
        If scalar: bin size for creating histogram bins
        If array: explicit bin edges to use
    debug : bool
        If True, print debugging information
    lambda_smooth : float
        Smoothing parameter (Jeffreys prior uses 0.5, Laplace uses 1.0)
    
    Returns:
    --------
    float
        Log-likelihood of the data under the Dirichlet-multinomial model
    """
    from scipy.special import gammaln
    
    # Get death times and events for sr2 (simulation)
    death_times2 = sr2.getDeathTimes()
    events2 = sr2.events
    
    # Restrict to time_range if provided
    if time_range is not None:
        mask2 = (death_times2 >= time_range[0]) & (death_times2 <= time_range[1])
        events2 = events2[mask2]
        death_times2 = death_times2[mask2]
    
    # Check if we have enough data from simulation
    if len(death_times2) <= 5:
        return -np.inf
    
    # Get death times and events for sr1 (data)
    death_times1 = sr1.getDeathTimes()
    events1 = sr1.events
    
    # Restrict to time_range if provided
    if time_range is not None:
        mask1 = (death_times1 >= time_range[0]) & (death_times1 <= time_range[1])
        events1 = events1[mask1]
        death_times1 = death_times1[mask1]
    
    # ============================================================================
    # Step 1: Define bins
    # ============================================================================
    # If dt is an array, use it as bin edges directly
    if isinstance(dt, (list, tuple, np.ndarray)):
        bins = np.array(dt)
        if debug:
            print(f"Using provided bin edges: {bins}")
    else:
        # dt is a scalar - create bins with this bin size
        # Determine the range for binning based on data (sr1)
        if time_range is not None:
            min_time = time_range[0]
            max_time_candidate = time_range[1]
        else:
            min_time = 0
            max_time_candidate = np.inf
        
        # Check the maximum death time in data (sr1)
        max_death_time_data = np.max(death_times1) if len(death_times1) > 0 else 0
        
        # If time_range is None or its right end is bigger than max death time in data,
        # use max death time from data
        if max_time_candidate > max_death_time_data:
            max_time = max_death_time_data
        else:
            max_time = max_time_candidate
        
        # Create bins from min_time to max_time with step dt
        # The last bin will be [max_time, +inf) to capture overflow
        bins = np.arange(min_time, max_time + dt, dt)
        
        if debug:
            print(f"Created bins from {min_time} to {max_time} with dt={dt}")
            print(f"Bins: {bins}")
    
    # The tail threshold T is the last bin edge
    # Everything >= T goes into the overflow bin
    T = bins[-1]
    
    # ============================================================================
    # Step 2: Compute counts in bins for simulation data (sr2)
    # ============================================================================
    # For non-censored (events2 == 1), histogram their death times
    died2 = death_times2[events2 == 1]
    
    # Use numpy histogram to count deaths in bins [edge_i, edge_{i+1})
    c_bins, _ = np.histogram(died2, bins=bins)
    
    # Count deaths in the tail (overflow bin): deaths >= T
    c_tail = np.sum(died2 >= T)
    
    # The total number of bins is len(bins) (including the overflow bin)
    # We fold the tail count into the last bin
    # c_bins has length len(bins)-1. We need to add the overflow bin.
    # According to pseudocode: fold tail into last bin
    if len(c_bins) > 0:
        # Overflow bin gets count from c_tail
        # Note: c_bins[-1] already contains counts in [bins[-2], bins[-1])
        # We add counts >= bins[-1] to create the final overflow bin
        c = np.concatenate([c_bins[:-1], [c_bins[-1] + c_tail]])
    else:
        c = np.array([c_tail])
    
    # Total number of non-censored events in simulation
    n_sim = len(died2)
    
    if debug:
        print(f"Simulation counts in bins (c): {c}")
        print(f"Total simulation events (n_sim): {n_sim}")
    
    # ============================================================================
    # Step 3: Compute counts in bins for data (sr1)
    # ============================================================================
    # For non-censored data (events1 == 1), histogram their death times
    died1 = death_times1[events1 == 1]
    
    y_bins, _ = np.histogram(died1, bins=bins)
    y_tail = np.sum(died1 >= T)
    
    if len(y_bins) > 0:
        y = np.concatenate([y_bins[:-1], [y_bins[-1] + y_tail]])
    else:
        y = np.array([y_tail])
    
    m = len(died1)  # Total number of non-censored events in data
    
    if debug:
        print(f"Data counts in bins (y): {y}")
        print(f"Total data events (m): {m}")
    
    # ============================================================================
    # Step 4: Compute Dirichlet-multinomial likelihood for non-censored data
    # ============================================================================
    # alpha = c + lambda_smooth
    alpha = c + lambda_smooth
    A0 = np.sum(alpha)
    
    # Dirichlet-multinomial log-likelihood formula:
    # log L = gammaln(A0) - gammaln(m + A0) + sum(gammaln(y + alpha) - gammaln(alpha))
    
    if debug:
        # Calculate per-bin contributions for debugging only
        bin_contributions = gammaln(y + alpha) - gammaln(alpha)
        logL_uncensored = (gammaln(A0) - gammaln(m + A0) + np.sum(bin_contributions))
        
        print(f"alpha = c + lambda: {alpha}")
        print(f"A0 = sum(alpha): {A0}")
        print(f"\nUncensored data - Per-bin contributions to log-likelihood:")
        print(f"{'Bin':<6} {'y[i]':<8} {'c[i]':<8} {'alpha[i]':<10} {'Contribution':<15}")
        print("-" * 60)
        for i in range(len(bin_contributions)):
            print(f"{i:<6} {y[i]:<8.1f} {c[i]:<8.1f} {alpha[i]:<10.2f} {bin_contributions[i]:<15.6f}")
        print(f"\nConstant terms: gammaln(A0) - gammaln(m + A0) = {gammaln(A0) - gammaln(m + A0):.6f}")
        print(f"Sum of bin contributions: {np.sum(bin_contributions):.6f}")
        print(f"Log-likelihood (uncensored): {logL_uncensored}")
    else:
        # Fast path when debug is False - no intermediate arrays
        logL_uncensored = (gammaln(A0) - gammaln(m + A0) + 
                          np.sum(gammaln(y + alpha) - gammaln(alpha)))
    
    # ============================================================================
    # Step 5: Handle censored data
    # ============================================================================
    # Censored observations: events1 == 0
    censored1 = death_times1[events1 == 0]
    
    if len(censored1) > 0:
        # Compute alpha_prime = alpha + y (posterior parameters)
        alpha_prime = alpha + y
        alpha0_prime = np.sum(alpha_prime)
        
        # Compute tail cumulative sums: A[r] = sum_{i>=r} alpha_prime[i]
        # A[r] is the sum of alpha_prime from bin r to the last bin (inclusive)
        K = len(alpha_prime)  # Number of bins
        A = np.zeros(K)
        for r in range(K):
            A[r] = np.sum(alpha_prime[r:])
        
        # Compute L[r]: number of censored observations in each bin
        # For bin r, we need to count censored times that fall in that bin's interval
        # Bin r corresponds to interval [bins[r], bins[r+1]) for r < K-1
        # and [bins[-1], +inf) for r = K-1 (overflow bin)
        L = np.zeros(K)
        for r in range(K - 1):
            # Count censored times in [bins[r], bins[r+1])
            L[r] = np.sum((censored1 >= bins[r]) & (censored1 < bins[r + 1]))
        # Last bin (overflow): censored times >= T
        L[K - 1] = np.sum(censored1 >= T)
        
        # Compute censored likelihood contribution
        # Iterate from K down to 1 (in Python: from K-1 down to 0)
        logL_censored = 0.0
        T_after = 0  # Cumulative count of censored observations after current bin
        
        if debug:
            print(f"\nCensored counts by bin (L): {L}")
            print(f"alpha_prime = alpha + y: {alpha_prime}")
            print(f"alpha0_prime = sum(alpha_prime): {alpha0_prime}")
            print(f"Tail cumulative sums A: {A}")
            print(f"\nCensored data - Per-bin contributions to log-likelihood:")
            print(f"{'Bin':<6} {'L[r]':<8} {'A[r]':<12} {'T_after':<10} {'Contribution':<15}")
            print("-" * 65)
            
            # Debug path: calculate and display per-bin contributions
            for r in range(K - 1, -1, -1):  # From K-1 down to 0
                Lr = L[r]
                if Lr == 0:
                    continue
                
                # Add censored contribution for bin r:
                # log L += gammaln(A[r] + Lr) - gammaln(A[r])
                #          - gammaln(alpha0_prime + T_after + Lr) + gammaln(alpha0_prime + T_after)
                bin_contrib = (gammaln(A[r] + Lr) - gammaln(A[r]) -
                              gammaln(alpha0_prime + T_after + Lr) + 
                              gammaln(alpha0_prime + T_after))
                
                print(f"{r:<6} {Lr:<8.1f} {A[r]:<12.2f} {T_after:<10.1f} {bin_contrib:<15.6f}")
                
                logL_censored += bin_contrib
                T_after += Lr
            
            print(f"\nLog-likelihood (censored): {logL_censored}")
        else:
            # Fast path when debug is False
            for r in range(K - 1, -1, -1):  # From K-1 down to 0
                Lr = L[r]
                if Lr == 0:
                    continue
                
                # Add censored contribution for bin r:
                # log L += gammaln(A[r] + Lr) - gammaln(A[r])
                #          - gammaln(alpha0_prime + T_after + Lr) + gammaln(alpha0_prime + T_after)
                logL_censored += (gammaln(A[r] + Lr) - gammaln(A[r]) -
                                 gammaln(alpha0_prime + T_after + Lr) + 
                                 gammaln(alpha0_prime + T_after))
                
                T_after += Lr
    else:
        logL_censored = 0.0
        if debug:
            print("No censored observations")
    
    # ============================================================================
    # Step 6: Total log-likelihood
    # ============================================================================
    total_logL = logL_uncensored + logL_censored
    
    if debug:
        print(f"\n{'='*65}")
        print(f"SUMMARY OF LOG-LIKELIHOOD CONTRIBUTIONS:")
        print(f"{'='*65}")
        print(f"Uncensored contribution:  {logL_uncensored:>15.6f}")
        print(f"Censored contribution:    {logL_censored:>15.6f}")
        print(f"{'-'*65}")
        print(f"Total log-likelihood:     {total_logL:>15.6f}")
        print(f"{'='*65}\n")
    
    # Check for NaN
    if np.isnan(total_logL):
        if debug:
            print("Warning: total log-likelihood is NaN")
        return -np.inf
    
    return total_logL


def lifetable_dirichlet_distance(life_table, sr2, time_range=None, lambda_smooth=0.5, debug=False):
    """
    Calculate the Dirichlet-multinomial likelihood that the life table data could be
    generated by the SR simulation model.
    
    This function uses a Dirichlet-multinomial model specifically designed for life table
    data where deaths are aggregated into age bins rather than observed individually.
    
    Key features:
    - Treats the last age as an open tail: [B,∞)
    - Builds bin edges from life table ages and appends tail edge
    - Calculates deaths per bin from n_alive differences
    - Handles right-censored observations (survivors at last age)
    - Supports time window conditioning with renormalization
    
    Parameters:
    -----------
    life_table : Life_table object
        The life table data (observations) containing ages, n_alive, and optional tail_bin
    sr2 : SR object
        The simulation SR object (model to evaluate)
    time_range : tuple, optional
        Time range (min, max) to restrict analysis. If provided, analyzes only bins
        within [t1, t2) and renormalizes within that window.
    lambda_smooth : float
        Smoothing parameter (Jeffreys prior uses 0.5, Laplace uses 1.0)
    debug : bool
        If True, print debugging information
    
    Returns:
    --------
    float
        Log-likelihood of the life table data under the Dirichlet-multinomial model
    """
    from scipy.special import gammaln
    
    # ============================================================================
    # Step 1: Extract life table data and determine tail edge
    # ============================================================================
    ages = life_table.ages
    n_alive = life_table.n_alive
    tail_bin = life_table.tail_bin
    
    # Validate life table has valid data
    if len(ages) == 0 or len(n_alive) == 0:
        if debug:
            print("Error: Empty life table data")
        return -np.inf
    
    if len(ages) != len(n_alive):
        if debug:
            print(f"Error: ages ({len(ages)}) and n_alive ({len(n_alive)}) must have same length")
        return -np.inf
    
    # Determine tail edge B
    if tail_bin is not None:
        B = tail_bin['age']  # Tail edge from explicit tail_bin
        n_survivors = tail_bin['n_alive']  # Right-censored count
        has_open_age = True
    else:
        # No explicit tail bin - treat last age as tail edge
        B = ages[-1]
        n_survivors = n_alive[-1]  # Survivors at last age
        has_open_age = False
    
    if debug:
        print(f"Life table ages: {ages}")
        print(f"n_alive: {n_alive}")
        print(f"Tail edge B: {B}")
        print(f"Number of right-censored (survivors): {n_survivors}")
        print(f"Has explicit open age: {has_open_age}")
    
    # ============================================================================
    # Step 2: Build bin edges from life table ages
    # ============================================================================
    # For life tables, ages typically represent bin left edges, creating bins:
    # [ages[0], ages[1]), [ages[1], ages[2]), ..., [ages[-2], ages[-1])
    # Then we append the tail edge B to create: [ages[-1], B), [B, ∞)
    # OR if tail_bin exists and B > ages[-1], we get: ..., [ages[-1], B), [B, ∞)
    
    if len(ages) == 1:
        # Only one age point - create bins: [ages[0], B), [B, ∞)
        if B > ages[0]:
            # Two bins: [ages[0], B) and [B, ∞)
            bin_edges = np.array([ages[0], B, B])
        else:
            # B == ages[0] - unusual case, create single tail bin: [B, ∞)
            # This creates bin_edges = [B, B] which gives one bin [B, ∞) when we histogram
            bin_edges = np.array([B])
    else:
        # Use ages as bin left edges
        # Create finite bins from consecutive ages
        bin_edges_finite = np.array(ages)
        
        # Append tail edge B to create explicit tail bin [B,∞)
        # If B equals the last age, we still want a separate tail bin
        # so append B regardless (it creates an empty finite bin [ages[-1], B) if B == ages[-1])
        bin_edges = np.concatenate([bin_edges_finite, [B]])
    
    if debug:
        print(f"Bin edges (finite + tail): {bin_edges}")
        print(f"Number of bins (including tail): {len(bin_edges) - 1}")
    
    # ============================================================================
    # Step 3: Apply time_range window if provided
    # ============================================================================
    # If time_range is provided, we'll filter bins and renormalize within the window
    # This will be handled when computing counts
    if time_range is not None:
        t1, t2 = time_range
        if debug:
            print(f"Time window: [{t1}, {t2})")
    else:
        t1, t2 = None, None
    
    # ============================================================================
    # Step 4: Calculate data counts from life table
    # ============================================================================
    # Deaths per finite bin: y_i = n_alive[i] - n_alive[i+1]
    # For bins constructed from ages, we have len(ages)-1 finite bins (if len(ages) > 1)
    # Then we add the tail bin
    
    n_finite_bins = len(ages) - 1 if len(ages) > 1 else 0
    
    if n_finite_bins > 0:
        # Calculate deaths in each finite bin: [ages[i], ages[i+1])
        y_finite = np.zeros(n_finite_bins)
        for i in range(n_finite_bins):
            y_finite[i] = n_alive[i] - n_alive[i + 1]
        
        # Check if there's a gap between last age and tail edge B
        # If B > ages[-1], we may have an additional finite bin [ages[-1], B)
        # But deaths in that interval would be unknown (0), so we can combine it with tail
        # For simplicity, we treat everything from ages[-1] to B as part of the transition
        # and explicitly create the tail bin at B
        
        # Note: If bin_edges has len(ages)+1 elements, the last finite bin is [ages[-1], B)
        # and deaths there would also be 0 (unknown). We combine it conceptually with tail.
        
    elif len(ages) == 1:
        # Only one age point - create one finite bin [ages[0], B) with 0 deaths
        # and tail bin [B, ∞)
        y_finite = np.array([0.0])
        n_finite_bins = 1
    else:
        y_finite = np.array([])
    
    # Tail bin: y_tail = 0 (unknown deaths in tail)
    y_tail = 0.0
    
    # Combine finite bins and tail bin
    # Total number of bins = n_finite_bins + 1 (tail)
    if n_finite_bins > 0:
        y = np.concatenate([y_finite, [y_tail]])
    else:
        # Edge case: no finite bins, only tail
        y = np.array([y_tail])
    
    m = np.sum(y_finite)  # Total deaths (excluding tail, which is 0)
    
    # Apply time window filtering if needed
    if time_range is not None:
        t1, t2 = time_range
        # Find which bins are within or intersect the window
        # Bins are: [bin_edges[i], bin_edges[i+1]) for i = 0 to len(bin_edges)-2
        # The tail bin is the last bin: [bin_edges[-2], bin_edges[-1]) = [bin_edges[-2], ∞)
        valid_bin_indices = []
        tail_bin_idx = len(bin_edges) - 2  # Index of tail bin (last bin)
        
        for i in range(len(bin_edges) - 1):
            bin_start = bin_edges[i]
            # For tail bin (last bin), bin_end is infinity
            bin_end = bin_edges[i + 1] if i < len(bin_edges) - 2 else np.inf
            # Bin intersects window if: bin_start < t2 and bin_end > t1
            if bin_start < t2 and bin_end > t1:
                valid_bin_indices.append(i)
        
        if len(valid_bin_indices) == 0:
            if debug:
                print(f"Warning: No bins intersect time window [{t1}, {t2})")
            return -np.inf
        
        # Check if tail bin is included BEFORE filtering (needed for n_survivors handling)
        tail_bin_included = tail_bin_idx in valid_bin_indices
        
        # Filter y to only include valid bins
        y_filtered = y[valid_bin_indices]
        
        # Also need to filter bin_edges - keep edges for valid bins
        # Build new bin_edges from the valid bin indices
        valid_bin_edges = [bin_edges[valid_bin_indices[0]]]
        for i, idx in enumerate(valid_bin_indices[1:], start=1):
            # The next edge for bin idx is bin_edges[idx + 1]
            # But if this is the tail bin, we need to preserve the tail edge
            if idx == tail_bin_idx:  # This is the tail bin
                valid_bin_edges.append(bin_edges[-1])  # Keep tail edge (B)
            else:
                valid_bin_edges.append(bin_edges[idx + 1])
        
        # Update to use filtered values
        y = y_filtered
        bin_edges = np.array(valid_bin_edges)
        m = np.sum(y[y > 0])  # Recalculate m from filtered data
        
        # Update n_survivors: only count if tail bin is included in window
        if not tail_bin_included:
            n_survivors = 0  # No censoring contribution if tail bin excluded
        
        if debug:
            print(f"After window filtering: {len(y)} bins, m = {m}")
            print(f"Filtered bin edges: {bin_edges}")
    
    if debug:
        print(f"Data counts (deaths per bin): y = {y}")
        print(f"Total deaths (m): {m}")
        print(f"Right-censored count: {n_survivors}")
    
    # ============================================================================
    # Step 5: Get simulation death times and filter
    # ============================================================================
    death_times2 = sr2.getDeathTimes()
    events2 = sr2.events
    
    # Restrict simulation to time_range if provided
    if time_range is not None:
        mask2 = (death_times2 >= time_range[0]) & (death_times2 <= time_range[1])
        events2 = events2[mask2]
        death_times2 = death_times2[mask2]
    
    # Check if we have enough simulation data
    if len(death_times2) <= 5:
        if debug:
            print(f"Warning: Insufficient simulation data ({len(death_times2)} observations)")
        return -np.inf
    
    # Get only deaths (non-censored) from simulation
    died2 = death_times2[events2 == 1]
    
    if len(died2) <= 5:
        if debug:
            print(f"Warning: Insufficient simulation deaths ({len(died2)} deaths)")
        return -np.inf
    
    if debug:
        print(f"Simulation: {len(died2)} deaths out of {len(death_times2)} observations")
    
    # ============================================================================
    # Step 6: Compute simulation counts in bins
    # ============================================================================
    # Histogram simulated deaths into the same bin edges (already filtered if time_range provided)
    
    # Get the tail edge (last bin edge, which is B)
    tail_edge = bin_edges[-1]
    
    # Histogram into all bins except tail (finite bins)
    if len(bin_edges) > 1:
        c_bins, _ = np.histogram(died2, bins=bin_edges[:-1])
        # Count all simulated deaths >= tail edge into tail bin
        c_tail = np.sum(died2 >= tail_edge)
        
        # Combine finite bins and tail bin
        c = np.concatenate([c_bins, [c_tail]])
    else:
        # Edge case: only one bin edge (shouldn't happen normally)
        c = np.array([np.sum(died2 >= tail_edge)])
    
    n_sim = len(died2)
    
    if debug:
        print(f"Simulation counts in bins: c = {c}")
        print(f"Total simulation deaths: {n_sim}")
    
    # ============================================================================
    # Step 7: Validate counts match after filtering
    # ============================================================================
    # Ensure y and c have the same length (same number of bins)
    if len(y) != len(c):
        if debug:
            print(f"Error: Mismatch in bin counts - y has {len(y)} bins, c has {len(c)} bins")
        return -np.inf
    
    # ============================================================================
    # Step 8: Compute Dirichlet-multinomial likelihood for uncensored data
    # ============================================================================
    # alpha = c + lambda_smooth
    alpha = c + lambda_smooth
    A0 = np.sum(alpha)
    
    # Dirichlet-multinomial log-likelihood formula:
    # log L = gammaln(A0) - gammaln(m + A0) + sum(gammaln(y + alpha) - gammaln(alpha))
    
    if debug:
        bin_contributions = gammaln(y + alpha) - gammaln(alpha)
        logL_uncensored = (gammaln(A0) - gammaln(m + A0) + np.sum(bin_contributions))
        
        print(f"\nUncensored data - Dirichlet-multinomial calculation:")
        print(f"alpha = c + lambda: {alpha}")
        print(f"A0 = sum(alpha): {A0}")
        print(f"\nPer-bin contributions:")
        print(f"{'Bin':<6} {'y[i]':<8} {'c[i]':<8} {'alpha[i]':<10} {'Contribution':<15}")
        print("-" * 60)
        for i in range(len(bin_contributions)):
            print(f"{i:<6} {y[i]:<8.1f} {c[i]:<8.1f} {alpha[i]:<10.2f} {bin_contributions[i]:<15.6f}")
        print(f"\nConstant: gammaln(A0) - gammaln(m + A0) = {gammaln(A0) - gammaln(m + A0):.6f}")
        print(f"Log-likelihood (uncensored): {logL_uncensored}")
    else:
        logL_uncensored = (gammaln(A0) - gammaln(m + A0) + 
                          np.sum(gammaln(y + alpha) - gammaln(alpha)))
    
    # ============================================================================
    # Step 9: Handle right-censored data (life-table survivors)
    # ============================================================================
    # Use life-table survivor count at last age as right-censored observations
    # These are censored at the tail edge B
    
    if n_survivors > 0:
        # Compute alpha_prime = alpha + y (posterior parameters)
        alpha_prime = alpha + y
        alpha0_prime = np.sum(alpha_prime)
        
        # Compute tail-cumulative sums: A[r] = sum_{i>=r} alpha_prime[i]
        K = len(alpha_prime)
        A = np.zeros(K)
        for r in range(K):
            A[r] = np.sum(alpha_prime[r:])
        
        # Censor mapping: censors at tail edge contribute to tail bin (last bin)
        # All n_survivors are censored at B, which goes to the tail bin (index K-1)
        L = np.zeros(K)
        L[K - 1] = n_survivors  # All censored observations go to tail bin
        
        # Compute censored likelihood contribution
        # Iterate from K-1 down to 0
        logL_censored = 0.0
        T_after = 0  # Cumulative count of censored observations after current bin
        
        if debug:
            print(f"\nCensored data - Right-censored contribution:")
            print(f"Number of right-censored: {n_survivors} at tail edge {B}")
            print(f"alpha_prime = alpha + y: {alpha_prime}")
            print(f"alpha0_prime = sum(alpha_prime): {alpha0_prime}")
            print(f"Tail cumulative sums A: {A}")
            print(f"Censored counts by bin (L): {L}")
            print(f"\nPer-bin censored contributions:")
            print(f"{'Bin':<6} {'L[r]':<8} {'A[r]':<12} {'T_after':<10} {'Contribution':<15}")
            print("-" * 65)
        
        for r in range(K - 1, -1, -1):  # From K-1 down to 0
            Lr = L[r]
            if Lr == 0:
                continue
            
            # Add censored contribution for bin r:
            # log L += gammaln(A[r] + Lr) - gammaln(A[r])
            #          - gammaln(alpha0_prime + T_after + Lr) + gammaln(alpha0_prime + T_after)
            bin_contrib = (gammaln(A[r] + Lr) - gammaln(A[r]) -
                          gammaln(alpha0_prime + T_after + Lr) + 
                          gammaln(alpha0_prime + T_after))
            
            if debug:
                print(f"{r:<6} {Lr:<8.1f} {A[r]:<12.2f} {T_after:<10.1f} {bin_contrib:<15.6f}")
            
            logL_censored += bin_contrib
            T_after += Lr
        
        if debug:
            print(f"\nLog-likelihood (censored): {logL_censored}")
    else:
        logL_censored = 0.0
        if debug:
            print("\nNo right-censored observations")
    
    # ============================================================================
    # Step 10: Total log-likelihood
    # ============================================================================
    total_logL = logL_uncensored + logL_censored
    
    if debug:
        print(f"\n{'='*65}")
        print(f"SUMMARY OF LOG-LIKELIHOOD CONTRIBUTIONS:")
        print(f"{'='*65}")
        print(f"Uncensored contribution:  {logL_uncensored:>15.6f}")
        print(f"Censored contribution:    {logL_censored:>15.6f}")
        print(f"{'-'*65}")
        print(f"Total log-likelihood:     {total_logL:>15.6f}")
        print(f"{'='*65}\n")
    
    # Check for NaN
    if np.isnan(total_logL):
        if debug:
            print("Warning: total log-likelihood is NaN")
        return -np.inf
    
    return total_logL


def distance(sr1, sr2, metric='baysian',time_range = None,**kwargs):
    """
    Calculate the distance between two SR models or SR model and equivalent data object according to different metrics.
    Currently only KL_divergence is supported.
    """
    if metric == 'survival':
        #calculate the mse between the two survival curves
        t1,s1 = sr1.getSurvival()
        t1,s1 = trim_to_range(t1,s1,time_range,renormalize_survival=True) if time_range is not None else (t1,s1)
        t2,s2 = sr2.getSurvival()
        t2,s2 = trim_to_range(t2,s2,time_range,renormalize_survival=True) if time_range is not None else (t2,s2)
        #if t2, s2 are empty return inf
        if len(t2) == 0:
            return np.inf
        s2_interp = np.interp(t1,t2,s2)
        distance = np.mean((s1-s2_interp)**2)
        return distance
    if metric == 'log_hazard':
        #calculate the mse between the two survival curves
        t1,h1 = sr1.getHazard()
        t1,h1 = trim_to_range(t1,h1,time_range) if time_range is not None else (t1,h1)
        t2,h2 = sr2.getHazard()
        t2,h2 = trim_to_range(t2,h2,time_range) if time_range is not None else (t2,h2)
        #if t2, s2 are empty return inf
        if len(t2) == 0:
            return np.inf
        t1s,s1 = sr1.getSurvival()
        t1s,s1 = trim_to_range(t1s,s1,time_range) if time_range is not None else (t1s,s1)
        t2s,s2 = sr2.getSurvival()
        t2s,s2 = trim_to_range(t2s,s2,time_range) if time_range is not None else (t2s,s2)
        s1_interp = np.interp(t1,t1s,s1)
        h2_interp = np.interp(t1,t2,h2)
        n_alive = sr1.npeople*s1_interp
        w = np.sqrt(n_alive)
        distance = np.mean(w*((np.log(h1)-np.log(h2_interp))**2))
        return distance
    if metric == 'baysian':
        #if n_bootstrapa is in kwargs use it, otherwise use 1
        dt = kwargs.get('dt',1)
        return baysianDistance(sr1, sr2, time_range=time_range, dt=dt)
    if metric == 'dirichlet':
        dt = kwargs.get('dt',1)
        lambda_smooth = kwargs.get('lambda_smooth',0.5)
        return baysian_dirichlet_distance(sr1, sr2, time_range=time_range, dt=dt, 
                                         debug=kwargs.get('debug',False), lambda_smooth=lambda_smooth)
    if metric == 'lifetable_dirichlet':
        lambda_smooth = kwargs.get('lambda_smooth',0.5)
        # sr1 should be a Life_table object
        return lifetable_dirichlet_distance(sr1, sr2, time_range=time_range,
                                           debug=kwargs.get('debug',False), lambda_smooth=lambda_smooth)
    if metric.startswith('KS') :
        death_times1 = sr1.death_times
        death_times2 = sr2.death_times
        #check death_times are not empty
        if len(death_times1) == 0 or len(death_times2) == 0:
            distance = np.inf
            p_value = 0
        else:
            ks_result = ks_2samp(death_times1, death_times2)
            distance = ks_result.statistic
            p_value = ks_result.pvalue
        if metric == 'KS_p_value':
            return distance, p_value
    
    if metric == 'KL_divergence':
        #note that the distributions should be non 0  for all values, currently I dont test this which gives nonsensical results
        #I need to add a test for this, maybe take the middle of the distribution...
        bins = np.linspace(0,80,81 )
        p1, bin_edges1 =sr1.get_death_times_distribution(bins=bins)
        p2, bin_edges2 =sr2.get_death_times_distribution(bins=bins)
        distance = entropy(p1,p2)
    else:
        raise ValueError('metric should be KL_divergence')

    
    return distance

